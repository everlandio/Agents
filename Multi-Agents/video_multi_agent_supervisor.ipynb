{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, Tuple, Union\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "import threading\n",
    "import base64\n",
    "import os\n",
    "\n",
    "@tool\n",
    "def get_video_description(\n",
    "    question: str, path: str\n",
    "    ):\n",
    "    \"\"\"Useful for descripting scenes and answer questions from a sequence of frames. Need a question and the path to the frames.\"\"\"\n",
    "    base64Frames = []\n",
    "    for filename in os.listdir(path):\n",
    "        with open(os.path.join(path, filename), \"rb\") as image_file:\n",
    "            base64Frames.append(base64.b64encode(image_file.read()).decode(\"utf-8\"))\n",
    "    llm = ChatOpenAI(model=\"gpt-4-vision-preview\", max_tokens=1028)\n",
    "    PROMPT_MESSAGES = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are provided with a sequence of frames from a video in base64 format. Your task is to analyze that sequence and answer questions related to them.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                question,\n",
    "                *map(lambda x: {\"image\": x, \"resize\": 768}, base64Frames[0::50]),\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    response = llm.invoke(PROMPT_MESSAGES)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "@tool\n",
    "def split_video(\n",
    "    video_path: str\n",
    "    ):\n",
    "    \"\"\"Split a video into 2 sets of frames, save them to analysts' folders and return the names of the folders.\"\"\"\n",
    "    print(\"success\")\n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "    success,image = vidcap.read()\n",
    "    count = 0\n",
    "    frames = []\n",
    "    # create the folders for the analysts\n",
    "    os.makedirs(\"analyst1_frames\", exist_ok=True)\n",
    "    os.makedirs(\"analyst2_frames\", exist_ok=True)\n",
    "    while success:\n",
    "        frames.append(image)\n",
    "        success,image = vidcap.read()\n",
    "        count += 1\n",
    "    # Divide the frames into 2 sets\n",
    "    frames1 = frames[:len(frames)//2]\n",
    "    frames2 = frames[len(frames)//2:]\n",
    "    # Save the frames to analysts' folders\n",
    "    for i, frame in enumerate(frames1):\n",
    "        cv2.imwrite(f\"analyst1_frames/{i}.jpg\", frame)\n",
    "    for i, frame in enumerate(frames2):\n",
    "        cv2.imwrite(f\"analyst2_frames/{i}.jpg\", frame)\n",
    "    return [\"analyst1_frames\", \"analyst2_frames\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def create_agent(\n",
    "    llm: ChatOpenAI, tools: list, system_prompt: str\n",
    "):\n",
    "    # Each worker node will be given a name and some tools.\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                system_prompt,\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "        ]\n",
    "    )\n",
    "    print(tools)\n",
    "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools)\n",
    "    return executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "\n",
    "members = [\"analyst_1\", \"analyst_2\", \"supervisor\"]\n",
    "\n",
    "system_prompt = (\n",
    "    \"Your role is to oversee a task involving the analysis of a video. Your responsibilities are as follows:\"\n",
    "\n",
    "    \"1. Video Division:\"\n",
    "    \"- You are provided with a video file. Your first task is to break this video into two equal parts. You are provided split_video tool to help you with this task.\"\n",
    "    \"- Ensure that the division is done in such a way that the two parts are contiguous and cover the entire video without overlapping.\"\n",
    "    \"- The next step begins once the video has been successfully divided.\"\n",
    "\n",
    "    \"2. Assigning Video Parts to Agents:\"\n",
    "    \"- You have two agents under your supervision, analyst_1 and analyst_2.\"\n",
    "    \"- Assign the first half of the video to analyst_1 and the second half to analyst_2.\"\n",
    "    \"- Provide each agent with their respective video part and instruct them to analyze their assigned section.\"\n",
    "\n",
    "    \"3. Gathering and Summarizing Responses:\"\n",
    "    \"- Once both agents have completed their analysis, they will provide you with their findings.\"\n",
    "    \"- Your task is to compile these findings into a comprehensive summary.\"\n",
    "    \"- The summary should include key points from each agent's analysis, highlighting any significant observations or conclusions.\"\n",
    "\n",
    "    \"4. Reporting:\"\n",
    "    \"- Prepare a final report that includes the following:\"\n",
    "        \"a. A brief description of each video part.\"\n",
    "        \"b. The individual summaries from analyst_1 and analyst_2.\"\n",
    "        \"c. Your comprehensive summary combining insights from both agents.\"\n",
    "    \"- Ensure that the report is clear, concise, and provides a complete understanding of the video's content as analyzed by the agents.\"\n",
    "\n",
    "    \"Remember, your role is crucial in ensuring that the video is thoroughly analyzed and the findings are accurately reported. Maintain clear communication with the agents and ensure that the workflow is followed as described.\"\n",
    "    \"When finished, respond with FINISH.\"\n",
    ")\n",
    "# Our team supervisor is an LLM node. It just picks the next agent to process\n",
    "# and decides when the work is completed\n",
    "options = [\"FINISH\"] + members\n",
    "# Using openai function calling can make output parsing easier for us\n",
    "function_def = {\n",
    "    \"name\": \"route\",\n",
    "    \"description\": \"Select the next role.\",\n",
    "    \"parameters\": {\n",
    "        \"title\": \"routeSchema\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"next\": {\n",
    "                \"title\": \"Next\",\n",
    "                \"anyOf\": [\n",
    "                    {\"enum\": options},\n",
    "                ],\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"next\"],\n",
    "    },\n",
    "}\n",
    "# split_video_def = {\n",
    "#     \"name\": \"split_video\",\n",
    "#     \"description\": \"Split a video into 2 sets of frames, save them to analysts' folders, and return the names of the folders.\",\n",
    "#     \"parameters\": {\n",
    "#         \"title\": \"video_path\",\n",
    "#         \"type\": \"object\",\n",
    "#         \"description\": \"The file path of the video to be split.\",\n",
    "#         \"properties\": {\n",
    "#             \"video_path\": {\n",
    "#                 \"type\": \"string\",\n",
    "#                 \"description\": \"The file path of the video to be split.\"\n",
    "#             }\n",
    "#         },\n",
    "#     },\n",
    "#     \"return\": {\n",
    "#         \"type\": \"list\",\n",
    "#         \"description\": \"A list containing the names of the two folders where the frames are saved.\"\n",
    "#     },\n",
    "#     \"required\": [\"video_path\"]\n",
    "# }\n",
    "\n",
    "# def next_agent(state):\n",
    "#     llm = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "#     options = [\"FINISH\", \"analyst_1\", \"analyst_2\", \"supervisor\"]\n",
    "#     prompt = ChatPromptTemplate.from_messages(\n",
    "#         [\n",
    "#             (\n",
    "#                 \"system\",\n",
    "#                 \"Given the conversation above, who should act next?\"\n",
    "#                 \" Or should we FINISH? Select one of: {options}\",\n",
    "#             ),\n",
    "#             (\"user\", \"{input}\"),\n",
    "#             MessagesPlaceholder(variable_name=\"messages\"),\n",
    "#             MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "#         ]\n",
    "#     ).partial(options=str(options))\n",
    "#     result = llm.invoke(state, prompt=prompt)\n",
    "#     return {\"messages\": [HumanMessage(content=result[\"output\"], name=\"supervisor\")]}\n",
    "    \n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next?\"\n",
    "            \" Or should we FINISH? Select one of: {options}\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ").partial(options=str(options), members=\", \".join(members))\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "tools=[split_video]\n",
    "# supervisor = create_openai_functions_agent(llm, tools=[function_def, split_video_def], prompt=prompt)\n",
    "# supervisor = AgentExecutor(agent=supervisor, tools=tools)\n",
    "supervisor = create_agent(llm, tools, system_prompt)\n",
    "\n",
    "# supervisor_chain = (\n",
    "#     prompt\n",
    "#     | llm.bind_tools(tools=[split_video_def, function_def])\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, Any, Dict, List, Optional, Sequence, TypedDict\n",
    "import functools\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "\n",
    "# The agent state is the input to each node in the graph\n",
    "class AgentState(TypedDict):\n",
    "    # The annotation tells the graph that new messages will always\n",
    "    # be added to the current states\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    video_path: str\n",
    "    # tool_call: Optional[Dict[str, Any]]\n",
    "    # The 'next' field indicates where to route to next\n",
    "\n",
    "prompt_analysts = (\n",
    "    \"You are a helpful AI assistant that answers questions about a video. You are provided with a list of frames and a question. Your task is to analyze the video and provide a response to the question.\"\n",
    "    \" Use the provided tool to see the video and progress towards answering the question.\"\n",
    ")\n",
    "\n",
    "analyst_1 = create_agent(llm, [get_video_description], prompt_analysts + \"You have access to the first part of the movie stored as frames. It is stored in the analyst1_frames folder.\")\n",
    "analyst_1_node = functools.partial(agent_node, agent=analyst_1, name=\"analyst_1\")\n",
    "\n",
    "analyst_2 = create_agent(llm, [get_video_description], prompt_analysts + \"You have access to the second part of the movie stored as frames. It is stored in the analyst2_frames folder.\")\n",
    "analyst_2_node = functools.partial(agent_node, agent=analyst_2, name=\"analyst_2\")\n",
    "\n",
    "supervisor_node = functools.partial(agent_node, agent=supervisor, name=\"supervisor\")\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"analyst_1\", analyst_1_node)\n",
    "workflow.add_node(\"analyst_2\", analyst_2_node)\n",
    "workflow.add_node(\"supervisor\", supervisor_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state):\n",
    "    # This is the router\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if \"FINAL ANSWER\" in last_message.content and last_message.name==\"supervisor\":\n",
    "        return \"end\"\n",
    "    if last_message.name == \"supervisor\" and messages[-2].name==\"user\":\n",
    "        return \"supervisor\"\n",
    "    if last_message.name == \"supervisor\" and messages[-2].name==\"analyst_1\":\n",
    "        return \"analyst_2\"\n",
    "    if last_message.name == \"supervisor\" and messages[-2].name==\"analyst_2\":\n",
    "        return \"analyst_1\"\n",
    "    if last_message.name == \"analyst_1\" or last_message.name == \"analyst_2\":\n",
    "        return \"supervisor\"\n",
    "    if last_message.name == \"supervisor\" and messages[-2].name==\"supervisor\":\n",
    "        return \"analyst_1\"\n",
    "    return \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for member in members:\n",
    "    workflow.add_edge(member, \"supervisor\")\n",
    "conditional_map = {k: k for k in members}\n",
    "conditional_map[\"FINISH\"] = END\n",
    "# conditional_map[\"function_call\"] = \"function_call\"\n",
    "# workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\", \n",
    "    router, \n",
    "    {\"supervisor\": \"supervisor\", \"analyst_1\": \"analyst_1\", \"analyst_2\": \"analyst_2\", \"FINISH\": END},\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"analyst_1\", \n",
    "    router, \n",
    "    {\"supervisor\": \"supervisor\", \"analyst_1\": \"analyst_1\", \"analyst_2\": \"analyst_2\"},\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"analyst_2\", \n",
    "    router, \n",
    "    {\"supervisor\": \"supervisor\", \"analyst_1\": \"analyst_1\", \"analyst_2\": \"analyst_2\"},\n",
    ")\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Write a brief report for the video. It's path is pizza.mp4.\", name=\"user\")\n",
    "        ],\n",
    "        \"video_path\": \"pizza.mp4\",\n",
    "    },\n",
    "    {\"recursion_limit\": 100},\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
