{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from typing import List, Optional\n",
    "\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, ToolMessage\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        reflection: Reflection,\n",
    "        parent: Optional[Node] = None,\n",
    "    ):\n",
    "        self.messages = messages\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.value = 0\n",
    "        self.visits = 0\n",
    "        self.reflection = reflection\n",
    "        self.depth = parent.depth + 1 if parent is not None else 1\n",
    "        self._is_solved = reflection.found_solution if reflection else False\n",
    "        if self._is_solved:\n",
    "            self._mark_tree_as_solved()\n",
    "        self.backpropagate(reflection.normalized_score)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"<Node value={self.value}, visits={self.visits},\"\n",
    "            f\" solution={self.messages} reflection={self.reflection}/>\"\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def is_solved(self):\n",
    "        \"\"\"If any solutions exist, we can end the search.\"\"\"\n",
    "        return self._is_solved\n",
    "\n",
    "    @property\n",
    "    def is_terminal(self):\n",
    "        return not self.children\n",
    "\n",
    "    @property\n",
    "    def best_child(self):\n",
    "        \"\"\"Select the child with the highest UCT to search next.\"\"\"\n",
    "        if not self.children:\n",
    "            return None\n",
    "        return max(self.children, key=lambda child: child.upper_confidence_bound())\n",
    "\n",
    "    @property\n",
    "    def best_child_score(self):\n",
    "        \"\"\"Return the child with the highest value.\"\"\"\n",
    "        if not self.children:\n",
    "            return None\n",
    "        return max(self.children, key=lambda child: int(child.is_solved) * child.value)\n",
    "\n",
    "    @property\n",
    "    def height(self) -> int:\n",
    "        \"\"\"Check for how far we've rolled out the tree.\"\"\"\n",
    "        if self.children:\n",
    "            return 1 + max([child.depth for child in self.children])\n",
    "        return 1\n",
    "\n",
    "    def upper_confidence_bound(self, exploration_weight=1.0):\n",
    "        \"\"\"Return the UCT score. This helps balance exploration vs. exploitation of a branch.\"\"\"\n",
    "        if self.parent is None:\n",
    "            raise ValueError(\"Cannot obtain UCT from root node\")\n",
    "        if self.visits == 0:\n",
    "            return self.value\n",
    "        # Encourages exploitation of high-value trajectories\n",
    "        average_reward = self.value / self.visits\n",
    "        # Encourages exploration of less-visited trajectories\n",
    "        exploration_term = math.sqrt(math.log(self.parent.visits) / self.visits)\n",
    "        return average_reward + exploration_weight * exploration_term\n",
    "\n",
    "    def backpropagate(self, reward: float):\n",
    "        \"\"\"Update the score of this node and its parents.\"\"\"\n",
    "        node = self\n",
    "        while node:\n",
    "            node.visits += 1\n",
    "            node.value = (node.value * (node.visits - 1) + reward) / node.visits\n",
    "            node = node.parent\n",
    "\n",
    "    def get_messages(self, include_reflections: bool = True):\n",
    "        if include_reflections:\n",
    "            return self.messages + [self.reflection.as_message()]\n",
    "        return self.messages\n",
    "\n",
    "    def get_trajectory(self, include_reflections: bool = True) -> List[BaseMessage]:\n",
    "        \"\"\"Get messages representing this search branch.\"\"\"\n",
    "        messages = []\n",
    "        node = self\n",
    "        while node:\n",
    "            messages.extend(\n",
    "                node.get_messages(include_reflections=include_reflections)[::-1]\n",
    "            )\n",
    "            node = node.parent\n",
    "        # Reverse the final back-tracked trajectory to return in the correct order\n",
    "        return messages[::-1]  # root solution, reflection, child 1, ...\n",
    "\n",
    "    def get_best_solution(self):\n",
    "        \"\"\"Return the best solution from within the current sub-tree.\"\"\"\n",
    "        all_nodes = [self]\n",
    "        nodes = deque()\n",
    "        nodes.append(self)\n",
    "        while nodes:\n",
    "            node = nodes.popleft()\n",
    "            all_nodes.extend(node.children)\n",
    "            for n in node.children:\n",
    "                nodes.append(n)\n",
    "        best_node = max(\n",
    "            all_nodes,\n",
    "            # We filter out all non-terminal, non-solution trajectories\n",
    "            key=lambda node: int(node.is_terminal and node.is_solved) * node.value,\n",
    "        )\n",
    "        return best_node\n",
    "\n",
    "    def _mark_tree_as_solved(self):\n",
    "        parent = self.parent\n",
    "        while parent:\n",
    "            parent._is_solved = True\n",
    "            parent = parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class TreeState(TypedDict):\n",
    "    # The full tree\n",
    "    root: Node\n",
    "    # The original input\n",
    "    input: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def encode_image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        encoded_string = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "    return encoded_string\n",
    "\n",
    "# Example usage\n",
    "# image_path = \"thumbnail.jpg\"\n",
    "# base64_image = encode_image_to_base64(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation\n",
    "\n",
    "@tool\n",
    "def get_image_description(question:str, image_path:str):\n",
    "    \"\"\"Useful for identifying points in an image that could be responsible for its bad or good CTR. It needs a question and the path to the image.\"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-4-vision-preview\", max_tokens=1028)\n",
    "    image = encode_image_to_base64(image_path)\n",
    "    return llm.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": \"{question}\"},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/png;base64,{image}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    ).content\n",
    "\n",
    "# @tool\n",
    "# def get_positive_points(image_path):\n",
    "#     \"\"\"Useful for identifying positive points in an image that could cause it to receive a good CTR.\"\"\"\n",
    "#     llm = ChatOpenAI(model=\"gpt-4-vision-preview\", max_tokens=1028)\n",
    "#     image = encode_image_to_base64(image_path)\n",
    "#     return llm.invoke(\n",
    "#         [\n",
    "#             HumanMessage(\n",
    "#                 content=[\n",
    "#                     {\"type\": \"text\", \"text\": \"Identify the positive points in this image that could cause it to receive a good CTR.\"},\n",
    "#                     {\n",
    "#                         \"type\": \"image_url\",\n",
    "#                         \"image_url\": {\"url\": f\"data:image/png;base64,{image}\"},\n",
    "#                     },\n",
    "#                 ]\n",
    "#             )\n",
    "#         ]\n",
    "#     ).content\n",
    "tools = [get_image_description]\n",
    "tool_executor = ToolExecutor(tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_structured_output_runnable\n",
    "from langchain.output_parsers.openai_tools import (\n",
    "    JsonOutputToolsParser,\n",
    "    PydanticToolsParser,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import chain as as_runnable\n",
    "\n",
    "\n",
    "class Reflection(BaseModel):\n",
    "    reflections: str = Field(\n",
    "        description=\"The critique and reflections on the sufficiency, superfluency,\"\n",
    "        \" and general quality of the response\"\n",
    "    )\n",
    "    score: int = Field(\n",
    "        description=\"Score from 0-10 on the quality of the candidate response.\",\n",
    "        gte=0,\n",
    "        lte=10,\n",
    "    )\n",
    "    found_solution: bool = Field(\n",
    "        description=\"Whether the response has fully solved the question or task.\"\n",
    "    )\n",
    "\n",
    "    def as_message(self):\n",
    "        return HumanMessage(\n",
    "            content=f\"Reasoning: {self.reflections}\\nScore: {self.score}\"\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def normalized_score(self) -> float:\n",
    "        return self.score / 10.0\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Reflect and grade the assistant response to the user question below.\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"candidate\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "reflection_llm_chain = (\n",
    "    prompt\n",
    "    | llm.bind_tools(tools=[Reflection], tool_choice=\"Reflection\").with_config(\n",
    "        run_name=\"Reflection\"\n",
    "    )\n",
    "    | PydanticToolsParser(tools=[Reflection])\n",
    ")\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def reflection_chain(inputs) -> Reflection:\n",
    "    tool_choices = reflection_llm_chain.invoke(inputs)\n",
    "    reflection = tool_choices[0]\n",
    "    if not isinstance(inputs[\"candidate\"][-1], AIMessage):\n",
    "        reflection.found_solution = False\n",
    "    return reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.prompt_values import ChatPromptValue\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, ValidationError\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful AI assistant that answers questions about images.\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "initial_answer_chain = prompt_template | llm.bind_tools(tools=tools).with_config(\n",
    "    run_name=\"GenerateInitialCandidate\"\n",
    ")\n",
    "\n",
    "\n",
    "parser = JsonOutputToolsParser(return_id=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_QjzQjRPHBsG1aGr18I9jh4gP', 'function': {'arguments': '{\"question\":\"Identify the positive and negative points in this image that may be important in predicting thumbnail CTR.\",\"image_path\":\"./thumbnail.jpg\"}', 'name': 'get_image_description'}, 'type': 'function'}]})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_response = initial_answer_chain.invoke(\n",
    "    {\"input\": \"Identify the positive and negative points in this image that may be important in predicting thumbnail CTR. Its path is ./thumbnainal.jpg.\"}\n",
    ")\n",
    "initial_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# Define the node we will add to the graph\n",
    "def generate_initial_response(state: TreeState) -> dict:\n",
    "    \"\"\"Generate the initial candidate response.\"\"\"\n",
    "    res = initial_answer_chain.invoke({\"input\": state[\"input\"]})\n",
    "    parsed = parser.invoke(res)\n",
    "    tool_responses = tool_executor.batch(\n",
    "        [ToolInvocation(tool=r[\"type\"], tool_input=r[\"args\"]) for r in parsed]\n",
    "    )\n",
    "    # print(tool_responses)\n",
    "    output_messages = [res] + [\n",
    "        ToolMessage(content=json.dumps(resp), tool_call_id=tool_call[\"id\"])\n",
    "        for resp, tool_call in zip(tool_responses, parsed)\n",
    "    ]\n",
    "    reflection = reflection_chain.invoke(\n",
    "        {\"input\": state[\"input\"], \"candidate\": output_messages}\n",
    "    )\n",
    "    root = Node(output_messages, reflection=reflection)\n",
    "    return {\n",
    "        **state,\n",
    "        \"root\": root,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This generates N candidate values\n",
    "# for a single input to sample actions from the environment\n",
    "\n",
    "\n",
    "def generate_candidates(messages: ChatPromptValue, config: RunnableConfig):\n",
    "    n = config[\"configurable\"].get(\"N\", 5)\n",
    "    bound_kwargs = llm.bind_tools(tools=tools).kwargs\n",
    "    chat_result = llm.generate(\n",
    "        [messages.to_messages()],\n",
    "        n=n,\n",
    "        callbacks=config[\"callbacks\"],\n",
    "        run_name=\"GenerateCandidates\",\n",
    "        **bound_kwargs\n",
    "    )\n",
    "    # print(f\"bound_kwargs: {bound_kwargs}\")\n",
    "    return [gen.message for gen in chat_result.generations[0]]\n",
    "\n",
    "\n",
    "expansion_chain = prompt_template | generate_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = expansion_chain.invoke({\"input\": \"Identify the positive and negative points in this image that may be important in predicting thumbnail CTR. Its path is ./thumbnainal.jpg.\"})\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def expand(state: TreeState, config: RunnableConfig) -> dict:\n",
    "    \"\"\"Starting from the \"best\" node in the tree, generate N candidates for the next step.\"\"\"\n",
    "    root = state[\"root\"]\n",
    "    best_candidate: Node = root.best_child if root.children else root\n",
    "    # print(f\"best_candidate: {best_candidate}\")\n",
    "    messages = best_candidate.get_trajectory()\n",
    "    # Generate N candidates from the single child candidate\n",
    "    new_candidates = expansion_chain.invoke(\n",
    "        {\"input\": state[\"input\"], \"messages\": messages}, config\n",
    "    )\n",
    "    parsed = parser.batch(new_candidates)\n",
    "    flattened = [\n",
    "        (i, tool_call)\n",
    "        for i, tool_calls in enumerate(parsed)\n",
    "        for tool_call in tool_calls\n",
    "    ]\n",
    "    tool_responses = tool_executor.batch(\n",
    "        [\n",
    "            ToolInvocation(tool=tool_call[\"type\"], tool_input=tool_call[\"args\"])\n",
    "            for _, tool_call in flattened\n",
    "        ]\n",
    "    )\n",
    "    collected_responses = defaultdict(list)\n",
    "    for (i, tool_call), resp in zip(flattened, tool_responses):\n",
    "        collected_responses[i].append(\n",
    "            ToolMessage(content=json.dumps(resp), tool_call_id=tool_call[\"id\"])\n",
    "        )\n",
    "    output_messages = []\n",
    "    for i, candidate in enumerate(new_candidates):\n",
    "        output_messages.append([candidate] + collected_responses[i])\n",
    "\n",
    "    # Reflect on each candidate\n",
    "    # For tasks with external validation, you'd add that here.\n",
    "    reflections = reflection_chain.batch(\n",
    "        [{\"input\": state[\"input\"], \"candidate\": msges} for msges in output_messages],\n",
    "        config,\n",
    "    )\n",
    "    # Grow tree\n",
    "    child_nodes = [\n",
    "        Node(cand, parent=best_candidate, reflection=reflection)\n",
    "        for cand, reflection in zip(output_messages, reflections)\n",
    "    ]\n",
    "    best_candidate.children.extend(child_nodes)\n",
    "    # We have already extended the tree directly, so we just return the state\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "\n",
    "def should_loop(state: TreeState):\n",
    "    \"\"\"Determine whether to continue the tree search.\"\"\"\n",
    "    root = state[\"root\"]\n",
    "    print(f\"state: {state}\")\n",
    "    if root.is_solved:\n",
    "        return END\n",
    "    if root.height > 5:\n",
    "        print(\"Reached max depth\")\n",
    "        return END\n",
    "    return \"expand\"\n",
    "\n",
    "\n",
    "builder = StateGraph(TreeState)\n",
    "builder.add_node(\"start\", generate_initial_response)\n",
    "builder.add_node(\"expand\", expand)\n",
    "builder.set_entry_point(\"start\")\n",
    "\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"start\",\n",
    "    # Either expand/rollout or finish\n",
    "    should_loop,\n",
    ")\n",
    "builder.add_conditional_edges(\n",
    "    \"expand\",\n",
    "    # Either continue to rollout or finish\n",
    "    should_loop,\n",
    ")\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "rolled out:  1\n",
      "---\n",
      "state: {'root': <Node value=0.9, visits=1, solution=[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_jPDx1MvVZlNuOwDUNtjkzlC8', 'function': {'arguments': '{\"question\":\"Identify the positive and negative points in this thumbnail image that could impact the thumbnail CTR.\",\"image_path\":\"./thumbnail.jpg\"}', 'name': 'get_image_description'}, 'type': 'function'}]}), ToolMessage(content='\"In the image, there is a person who appears to be shooting a firearm at a can of Coca-Cola, as indicated by the visual effects of a bullet hole and a muzzle flash. The text \\\\\"40M\\\\\" suggests some form of celebration or achievement, possibly related to reaching a milestone of 40 million in some context, which could be subscribers, views, or something else significant to the individual. The person is wearing protective hearing gear typically used during shooting for safety. The image is likely edited to dramatize the action for effect or to illustrate a story or achievement in a metaphorical way.\"', tool_call_id='call_jPDx1MvVZlNuOwDUNtjkzlC8')] reflection=reflections='The report provides a detailed description of the thumbnail image, highlighting the key elements that could impact the thumbnail CTR. It points out the positive aspects such as the visual effects, text indicating a milestone, and the use of protective gear for safety. Additionally, it mentions the potential negative impact of the dramatized action in the image. Overall, the report effectively analyzes the image from a CTR perspective.' score=9 found_solution=False/>, 'input': 'Make a detailed report identifing the positive and negative points in this thumbnail image that could impact the thumbnail CTR. Its path is ./thumbnainal.jpg.'}\n",
      "expand\n",
      "rolled out:  3\n",
      "---\n",
      "state: {'root': <Node value=0.3166666666666666, visits=6, solution=[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_jPDx1MvVZlNuOwDUNtjkzlC8', 'function': {'arguments': '{\"question\":\"Identify the positive and negative points in this thumbnail image that could impact the thumbnail CTR.\",\"image_path\":\"./thumbnail.jpg\"}', 'name': 'get_image_description'}, 'type': 'function'}]}), ToolMessage(content='\"In the image, there is a person who appears to be shooting a firearm at a can of Coca-Cola, as indicated by the visual effects of a bullet hole and a muzzle flash. The text \\\\\"40M\\\\\" suggests some form of celebration or achievement, possibly related to reaching a milestone of 40 million in some context, which could be subscribers, views, or something else significant to the individual. The person is wearing protective hearing gear typically used during shooting for safety. The image is likely edited to dramatize the action for effect or to illustrate a story or achievement in a metaphorical way.\"', tool_call_id='call_jPDx1MvVZlNuOwDUNtjkzlC8')] reflection=reflections='The report provides a detailed description of the thumbnail image, highlighting the key elements that could impact the thumbnail CTR. It points out the positive aspects such as the visual effects, text indicating a milestone, and the use of protective gear for safety. Additionally, it mentions the potential negative impact of the dramatized action in the image. Overall, the report effectively analyzes the image from a CTR perspective.' score=9 found_solution=False/>, 'input': 'Make a detailed report identifing the positive and negative points in this thumbnail image that could impact the thumbnail CTR. Its path is ./thumbnainal.jpg.'}\n",
      "expand\n",
      "rolled out:  3\n",
      "---\n",
      "state: {'root': <Node value=0.5636363636363636, visits=11, solution=[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_jPDx1MvVZlNuOwDUNtjkzlC8', 'function': {'arguments': '{\"question\":\"Identify the positive and negative points in this thumbnail image that could impact the thumbnail CTR.\",\"image_path\":\"./thumbnail.jpg\"}', 'name': 'get_image_description'}, 'type': 'function'}]}), ToolMessage(content='\"In the image, there is a person who appears to be shooting a firearm at a can of Coca-Cola, as indicated by the visual effects of a bullet hole and a muzzle flash. The text \\\\\"40M\\\\\" suggests some form of celebration or achievement, possibly related to reaching a milestone of 40 million in some context, which could be subscribers, views, or something else significant to the individual. The person is wearing protective hearing gear typically used during shooting for safety. The image is likely edited to dramatize the action for effect or to illustrate a story or achievement in a metaphorical way.\"', tool_call_id='call_jPDx1MvVZlNuOwDUNtjkzlC8')] reflection=reflections='The report provides a detailed description of the thumbnail image, highlighting the key elements that could impact the thumbnail CTR. It points out the positive aspects such as the visual effects, text indicating a milestone, and the use of protective gear for safety. Additionally, it mentions the potential negative impact of the dramatized action in the image. Overall, the report effectively analyzes the image from a CTR perspective.' score=9 found_solution=False/>, 'input': 'Make a detailed report identifing the positive and negative points in this thumbnail image that could impact the thumbnail CTR. Its path is ./thumbnainal.jpg.'}\n",
      "__end__\n",
      "rolled out:  3\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "question = \"Make a detailed report identifing the positive and negative points in this thumbnail image that could impact the thumbnail CTR. Its path is ./thumbnainal.jpg.\"\n",
    "for step in graph.stream({\"input\": question}, {\"recursion_limit\":150},):\n",
    "    step_name, step_state = next(iter(step.items()))\n",
    "    print(step_name)\n",
    "    print(\"rolled out: \", step_state[\"root\"].height)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize for the oversight. Let me provide a more detailed analysis of the positive and negative points in the thumbnail image that could impact the thumbnail CTR based on the description provided earlier:\n",
      "\n",
      "Positive Points:\n",
      "1. Visual Effects: The image includes visual effects such as a bullet hole and a muzzle flash, which can attract attention and create intrigue for viewers.\n",
      "2. Milestone Indicator: The text \"40M\" suggests a milestone achievement, which can generate curiosity and interest among viewers.\n",
      "3. Safety Gear: The presence of protective hearing gear indicates safety consciousness, which can be perceived positively by the audience.\n",
      "\n",
      "Negative Points:\n",
      "1. Dramatized Action: The dramatized action of shooting a firearm at a can of Coca-Cola may convey violence or aggression, potentially leading to a negative perception among some viewers.\n",
      "2. Context Ambiguity: The context of the image, with elements like the firearm and Coca-Cola can, may be unclear or confusing to some viewers, affecting the overall message conveyed.\n",
      "\n",
      "These points should be considered when evaluating the impact of the thumbnail image on the CTR. If you need further analysis or insights, feel free to ask.\n"
     ]
    }
   ],
   "source": [
    "solution_node = step[\"__end__\"][\"root\"].get_best_solution()\n",
    "best_trajectory = solution_node.get_trajectory(include_reflections=False)\n",
    "print(best_trajectory[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
